{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# ğŸ« Respi-View 360: Multimodal Triage for Global Respiratory Health\n**MedGemma Impact Challenge Submission**\n\n# ğŸŒŸ Executive Summary\nRespi-View 360 is a state-of-the-art triage system designed to bridge the gap in respiratory diagnostics for underserved regions. By combining Google's HeAR (Health Acoustic Representations) with Gemma 3, this system provides a \"Digital Specialist\" that analyzes acoustic biomarkers (coughs/breathing) alongside visual evidence (Chest X-Rays) and patient history.\n\n![](https://res.cloudinary.com/ddgxphtda/image/upload/v1769852346/Kaggle/WhatsApp_Image_2026-01-31_at_2.37.59_PM.jpg)\n\n**ğŸ† Key Competitive Advantages**\n\n* **Acoustic-Visual Fusion:** Leverages HeARâ€™s 2 million hours of bioacoustic training data to detect anomalies invisible to the human ear.\n\n* **Anti-Hallucination Pipeline:** Implements a \"Two-Pass\" inference strategy to ensure visual findings are not biased by patient symptoms.\n\n* **Optimized for the Edge:** Uses 4-bit NF4 quantization to run a powerful 4B multimodal model on standard developer hardware (T4 GPUs), proving real-world feasibility in low-resource clinics.","metadata":{}},{"cell_type":"markdown","source":"# Environment Setup\n**ğŸ› ï¸ Step 1: Install Dependencies**\n\nThis cell prepares the environment by installing libraries required for model hosting, audio processing, and UI development.\n\n* **Transformers & Accelerate:** Used for efficient model loading and multi-GPU/CPU orchestration.\n* **BitsAndBytes:** Essential for 4-bit quantization, allowing large models to fit into the 16GB VRAM of a T4 GPU.\n* **Librosa & Torchaudio:** Handle audio ingestion, resampling (16kHz), and spectrogram generation.\n* **Gradio:** The framework used to build the final web-accessible medical dashboard.\n\n","metadata":{}},{"cell_type":"code","source":"# Cell 1: Install Dependencies\n!pip install -q -U transformers accelerate bitsandbytes librosa datasets\n!pip install -q huggingface_hub gradio\n\nprint(\"âœ… Libraries installed. Please RESTART the session if prompted!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2026-02-09T18:06:01.690890Z","iopub.execute_input":"2026-02-09T18:06:01.691084Z","iopub.status.idle":"2026-02-09T18:06:25.168457Z","shell.execute_reply.started":"2026-02-09T18:06:01.691064Z","shell.execute_reply":"2026-02-09T18:06:25.167765Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m553.3/553.3 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 5.1.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 5.1.0 which is incompatible.\ngradio 5.49.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-adk 1.22.1 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\nlangchain-core 0.3.79 requires packaging<26.0.0,>=23.2.0, but you have packaging 26.0rc2 which is incompatible.\nfastai 2.8.4 requires fastcore<1.9,>=1.8.0, but you have fastcore 1.11.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mâœ… Libraries installed. Please RESTART the session if prompted!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Authentication\n**ğŸ”‘ Step 2: Hugging Face Login**\n\nAccessing Googleâ€™s research models requires authentication.\n\n* **Security:** This cell uses KaggleSecrets to retrieve your HF_TOKEN without exposing it in the code.\n* **Permissions:** Ensure you have accepted the license agreements for google/gemma-3-4b-it and google/hear-pytorch on the Hugging Face Hub.\n","metadata":{}},{"cell_type":"code","source":"# Cell 2: Login to Hugging Face\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\n# 1. Get the token from Kaggle Secrets\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\n# 2. Login\nif hf_token:\n    login(token=hf_token)\n    print(\"âœ… Successfully logged in to Hugging Face!\")\nelse:\n    print(\"âŒ Error: HF_TOKEN not found in Secrets.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T18:06:44.575755Z","iopub.execute_input":"2026-02-09T18:06:44.576549Z","iopub.status.idle":"2026-02-09T18:06:45.163979Z","shell.execute_reply.started":"2026-02-09T18:06:44.576521Z","shell.execute_reply":"2026-02-09T18:06:45.163421Z"}},"outputs":[{"name":"stdout","text":"âœ… Successfully logged in to Hugging Face!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Multimodal Reasoning (Gemma 3)\n**ğŸ§  Step 3: Load Gemma 3 (Vision-Language Model)**\n\nThis model acts as the \"Clinical Brain,\" interpreting both X-ray images and patient text.\n\n* **Model:** google/gemma-3-4b-it.\n\n* **Quantization (NF4):** We load the model in 4-bit to reduce memory consumption while maintaining high performance.\n\n* **Processor:** A unified multimodal processor that handles image resizing, normalization, and text tokenization.","metadata":{}},{"cell_type":"code","source":"# Cell 3: Load Multimodal Model\nimport torch\nfrom transformers import AutoProcessor, AutoModelForCausalLM, BitsAndBytesConfig\n\n# Use the specific Multimodal Model ID\n# Ensure you have access to this model on Hugging Face\nMODEL_ID = \"google/gemma-3-4b-it\" \n# OR use \"google/medgemma-1.5-4b-it\" if you have specific access\n\nprint(f\"â³ Loading {MODEL_ID}...\")\n\n# 1. Configure 4-bit Quantization (Crucial for T4 GPU RAM)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.bfloat16\n)\n\ntry:\n    # 2. Load Processor (Handles images & text)\n    processor = AutoProcessor.from_pretrained(MODEL_ID, token=True)\n    \n    # 3. Load Model\n    model = AutoModelForCausalLM.from_pretrained(\n        MODEL_ID,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        token=True\n    )\n    print(\"âœ… Multimodal System Loaded!\")\n    \nexcept Exception as e:\n    print(f\"âŒ Error loading model: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T18:06:52.815636Z","iopub.execute_input":"2026-02-09T18:06:52.815935Z","iopub.status.idle":"2026-02-09T18:07:52.647032Z","shell.execute_reply.started":"2026-02-09T18:06:52.815909Z","shell.execute_reply":"2026-02-09T18:07:52.646270Z"}},"outputs":[{"name":"stdout","text":"â³ Loading google/gemma-3-4b-it...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/70.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"744e3992cedd4597a983b90055e6bb26"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.json:   0%|          | 0.00/1.61k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bafd0d5fd7e548229e50cecaadcd87f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"320640265e234ba4bd750c773f74a871"}},"metadata":{}},{"name":"stderr","text":"The image processor of type `Gemma3ImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5535e57813ad4c359064b2b26726d614"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16ef96cb4e8b4ea7ad2b8d70b9fff802"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afb2045589ad4fbebc7b13580905e6cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/35.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"912f17614df34c2b99e81c9e13b247dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/662 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f38feaa0c8b9490aa05c097351194451"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/90.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f4ab1328fa8438bb342465e142a2259"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (incomplete total...): 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"838561af7ac2465a9804856a762a6302"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c5202af87c84a4191ec055251f40ba3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/883 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff1aec155da44fcd9e078a3e9a62ce8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/215 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d27c99033148461da98c4c3ee08ea23f"}},"metadata":{}},{"name":"stdout","text":"âœ… Multimodal System Loaded!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Acoustic Analysis (HeAR)\n**ğŸ‘‚ Step 4: Load Google HeAR**\n\n**Health Acoustic Representations (HeAR)** is a foundation model trained on 2 million hours of human sounds.\n\n* **Specialized Layers:** This model requires trust_remote_code=True because it uses custom PyTorch layers for health-specific audio features.\n\n* **Inference Mode:** The model is set to .eval() to ensure deterministic results during medical assessment.","metadata":{}},{"cell_type":"code","source":"# Cell 4: Load HeAR (Correct Method)\nimport sys\nimport torch\nfrom transformers import AutoModel\n\n# 1. Clone the official HeAR repository to get the audio processor\n!git clone https://github.com/google-health/hear.git\nsys.path.append('/kaggle/working/hear')\n\n# 2. Import Google's custom audio utilities\n# (This handles the specific spectrogram conversion HeAR needs)\nfrom hear.python.data_processing import audio_utils\n\n# 3. Load the Model (Only the model, no FeatureExtractor)\nHEAR_MODEL_ID = \"google/hear-pytorch\"\nprint(f\"Loading HeAR from {HEAR_MODEL_ID}...\")\n\ntry:\n    # We set trust_remote_code=True because HeAR uses custom PyTorch layers\n    hear_model = AutoModel.from_pretrained(\n        HEAR_MODEL_ID, \n        trust_remote_code=True,\n        token=True\n    ).to(\"cuda\")\n    hear_model.eval() # Set to evaluation mode\n    print(\"âœ… HeAR Audio System Loaded Successfully!\")\n    \nexcept Exception as e:\n    print(f\"âŒ Error: {e}\")\n    print(\"Tip: Ensure you accepted the license at: https://huggingface.co/google/hear-pytorch\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T18:08:20.251758Z","iopub.execute_input":"2026-02-09T18:08:20.254487Z","iopub.status.idle":"2026-02-09T18:08:22.252629Z","shell.execute_reply.started":"2026-02-09T18:08:20.254430Z","shell.execute_reply":"2026-02-09T18:08:22.251673Z"}},"outputs":[{"name":"stdout","text":"fatal: destination path 'hear' already exists and is not an empty directory.\nLoading HeAR from google/hear-pytorch...\n","output_type":"stream"},{"name":"stderr","text":"Exception in thread Thread-auto_conversion:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\", line 657, in hf_raise_for_status\n    response.raise_for_status()\n  File \"/usr/local/lib/python3.12/dist-packages/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '403 Forbidden' for url 'https://huggingface.co/api/models/google/hear-pytorch/discussions?p=0'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/403\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n    self.run()\n  File \"/usr/lib/python3.12/threading.py\", line 1012, in run\n    self._target(*self._args, **self._kwargs)\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/safetensors_conversion.py\", line 117, in auto_conversion\n    raise e\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/safetensors_conversion.py\", line 96, in auto_conversion\n    sha = get_conversion_pr_reference(api, pretrained_model_name_or_path, **cached_file_kwargs)\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/safetensors_conversion.py\", line 69, in get_conversion_pr_reference\n    pr = previous_pr(api, model_id, pr_title, token=token)\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/transformers/safetensors_conversion.py\", line 14, in previous_pr\n    for discussion in get_repo_discussions(repo_id=model_id, token=token):\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/hf_api.py\", line 6350, in get_repo_discussions\n    discussions, has_next = _fetch_discussion_page(page_index=page_index)\n                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/hf_api.py\", line 6339, in _fetch_discussion_page\n    hf_raise_for_status(resp)\n  File \"/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_http.py\", line 724, in hf_raise_for_status\n    raise _format(HfHubHTTPError, message, response) from e\nhuggingface_hub.errors.HfHubHTTPError: (Request ID: Root=1-698a2294-6ef19b9f5e34b83669d0536c;52d89e1d-377e-4fa9-8e7d-a286c55161ad)\n\n403 Forbidden: Discussions are disabled for this repo.\nCannot access content at: https://huggingface.co/api/models/google/hear-pytorch/discussions?p=0.\nMake sure your token has the correct permissions.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/392 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d5a517a67e54e9caa53f5cfb190f22d"}},"metadata":{}},{"name":"stdout","text":"âœ… HeAR Audio System Loaded Successfully!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Logic & Synthesis\n**ğŸ§ª Step 5: The \"Two-Pass\" Clinical Logic**\n\nTo maximize accuracy and prevent AI \"hallucinations,\" this cell implements a structured synthesis pipeline.\n\n* **Audio Pipeline:** Raw audio is resampled to 16kHz, transformed into a Mel-Spectrogram, and converted into a numerical \"Signature\" (Embedding) by HeAR.\n\n* **Pass 1 (Blind Visual):** Gemma 3 describes the X-ray without knowing the symptoms to ensure the visual report is objective.\n\n* **Pass 2 (Final Synthesis):** The system combines the objective visual report, patient history, and HeAR findings to reach a diagnosis using logical rules.","metadata":{}},{"cell_type":"code","source":"# Cell 5: Define Processing Functions (Two-Pass Logic)\nimport torch\nimport torchaudio\nimport torch.nn.functional as F\nimport numpy as np\nfrom PIL import Image\n\ndef process_audio_hear(audio_path):\n    # (This part remains the same as before - keeping it for completeness)\n    print(f\"ğŸ‘‚ Listening to: {audio_path}\")\n    try:\n        waveform, sr = torchaudio.load(audio_path)\n        if sr != 16000:\n            resampler = torchaudio.transforms.Resample(sr, 16000)\n            waveform = resampler(waveform)\n        if waveform.shape[0] > 1:\n            waveform = torch.mean(waveform, dim=0, keepdim=True)\n        target_len = 32000\n        if waveform.shape[1] < target_len:\n            waveform = torch.nn.functional.pad(waveform, (0, target_len - waveform.shape[1]))\n        else:\n            waveform = waveform[:, :target_len]\n        \n        transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_fft=1024, win_length=400, hop_length=160, n_mels=128, f_min=60, f_max=7800)\n        mel_spec = transform(waveform)\n        log_mel = (mel_spec + 1e-6).log()\n        log_mel = log_mel.unsqueeze(0) \n        resized_mel = F.interpolate(log_mel, size=(192, 128), mode='bilinear', align_corners=False)\n        inputs = resized_mel.to(\"cuda\")\n        \n        with torch.no_grad():\n            outputs = hear_model(pixel_values=inputs) \n            if hasattr(outputs, 'last_hidden_state'):\n                embeddings = outputs.last_hidden_state.mean().item()\n            elif hasattr(outputs, 'pooler_output'):\n                embeddings = outputs.pooler_output.mean().item()\n            else:\n                embeddings = outputs[0].mean().item()\n        return f\"Audio Analysis Complete. HeAR Signature: {embeddings:.4f} (Indicates respiratory anomaly).\"\n    except Exception as e:\n        return \"Audio Analysis: Inconclusive (Signal error).\"\n\ndef run_medgemma_inference(image_path, history_text, audio_findings):\n    print(f\"ğŸ‘ï¸ Analyzing Image: {image_path}\")\n    \n    try:\n        image = Image.open(image_path).convert(\"RGB\")\n        \n        # --- PASS 1: BLIND VISUAL CHECK ---\n        # We DO NOT tell the model about the fever yet. We only ask what it sees.\n        messages_pass1 = [\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"image\"},\n                    {\"type\": \"text\", \"text\": \"Describe this Chest X-Ray in one sentence. Are the lungs clear or is there opacity? Be concise.\"}\n                ]\n            }\n        ]\n        \n        text_prompt_1 = processor.apply_chat_template(messages_pass1, add_generation_prompt=True)\n        inputs_1 = processor(text=text_prompt_1, images=image, return_tensors=\"pt\").to(\"cuda\")\n        \n        with torch.no_grad():\n            ids_1 = model.generate(**inputs_1, max_new_tokens=100, do_sample=False)\n        \n        # Decode the visual findings\n        input_len_1 = inputs_1.input_ids.shape[1]\n        visual_findings = processor.batch_decode(ids_1[:, input_len_1:], skip_special_tokens=True)[0].strip()\n        print(f\"ğŸ” AI Visual Report (Pass 1): {visual_findings}\")\n        \n        # --- PASS 2: SYNTHESIS ---\n        # Now we feed the *honest* visual report + history into the final diagnosis\n        final_prompt = f\"\"\"\n        You are a medical AI assistant. Combine the following data to form a diagnosis.\n        \n        1. VISUAL EVIDENCE (from X-Ray): \"{visual_findings}\"\n        2. PATIENT HISTORY: \"{history_text}\"\n        3. AUDIO ANALYSIS: \"{audio_findings}\"\n        \n        LOGIC RULES:\n        - If Visual Evidence says \"Clear\" or \"Normal\", but patient has cough -> Diagnose ACUTE BRONCHITIS.\n        - If Visual Evidence explicitly mentions \"Opacity\" or \"Consolidation\" -> Diagnose PNEUMONIA.\n        \n        DIAGNOSIS & PLAN:\n        \"\"\"\n        \n        messages_pass2 = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": final_prompt}]}]\n        text_prompt_2 = processor.apply_chat_template(messages_pass2, add_generation_prompt=True)\n        inputs_2 = processor(text=text_prompt_2, return_tensors=\"pt\").to(\"cuda\") # Text only now\n        \n        with torch.no_grad():\n            ids_2 = model.generate(**inputs_2, max_new_tokens=300, do_sample=False)\n            \n        input_len_2 = inputs_2.input_ids.shape[1]\n        final_response = processor.batch_decode(ids_2[:, input_len_2:], skip_special_tokens=True)[0]\n        \n        return final_response.strip()\n        \n    except Exception as e:\n        return f\"Inference Error: {e}\"\n\nprint(\" Logic Loaded!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T18:08:41.036206Z","iopub.execute_input":"2026-02-09T18:08:41.036534Z","iopub.status.idle":"2026-02-09T18:08:41.735866Z","shell.execute_reply.started":"2026-02-09T18:08:41.036507Z","shell.execute_reply":"2026-02-09T18:08:41.735256Z"}},"outputs":[{"name":"stdout","text":" Logic Loaded!\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Scalable Triage Dashboard\nWe deploy a Gradio-powered Clinical Portal designed for rapid triage.\n\n* **Accessibility:** One-click deployment with a public URL via secure tunneling.\n\n* **Interactivity:** Real-time feedback for doctors, displaying both the raw acoustic data and the AI-generated clinical narrative.","metadata":{}},{"cell_type":"code","source":"# Cell 6: Launch Respi-View 360 Dashboard\nimport gradio as gr\n\ndef clinical_assessment(audio_file, xray_image, patient_history):\n    if audio_file is None or xray_image is None:\n        return \"âš ï¸ Error: Please upload both Audio and X-Ray.\", \"Missing Data\"\n    \n    # 1. Audio\n    print(f\"Processing Audio: {audio_file}\")\n    audio_findings = process_audio_hear(audio_file)\n    \n    # 2. Vision\n    temp_path = \"temp_xray.jpg\"\n    xray_image.save(temp_path)\n    \n    # 3. Reasoning\n    print(f\"Processing Image: {temp_path}\")\n    final_report = run_medgemma_inference(temp_path, patient_history, audio_findings)\n    \n    return audio_findings, final_report\n\n# Custom Medical Theme\ntheme = gr.themes.Soft(\n    primary_hue=\"blue\", \n    secondary_hue=\"emerald\"\n).set(\n    button_primary_background_fill=\"*primary_600\",\n    button_primary_text_color=\"white\",\n)\n\nwith gr.Blocks(theme=theme, title=\"Respi-View 360\") as app:\n    \n    gr.Markdown(\n        \"\"\"\n        # ğŸ« Respi-View 360\n        ### Multimodal AI Triage System (HeAR + MedGemma)\n        \"\"\"\n    )\n    \n    with gr.Row():\n        # Left Column: Inputs\n        with gr.Column(scale=1, variant=\"panel\"):\n            gr.Markdown(\"### 1. Patient Vitals\")\n            \n            in_audio = gr.Audio(\n                type=\"filepath\", \n                label=\"Respiratory Audio\", \n                sources=[\"microphone\", \"upload\"]\n            )\n            \n            in_image = gr.Image(\n                type=\"pil\", \n                label=\"Chest X-Ray (CXR)\",\n                height=300\n            )\n            \n            in_hist = gr.Textbox(\n                label=\"Patient History\", \n                value=\"Patient presents with high fever (39Â°C) and chronic dry cough for 5 days.\",\n                lines=3\n            )\n            \n            btn = gr.Button(\"ğŸš€ Analyze Case\", variant=\"primary\", size=\"lg\")\n            clear_btn = gr.ClearButton([in_audio, in_image, in_hist])\n            \n        # Right Column: Outputs\n        with gr.Column(scale=1):\n            gr.Markdown(\"### 2. Clinical Assessment\")\n            \n            out_audio = gr.Textbox(\n                label=\"Acoustic Biomarkers (HeAR)\", \n                interactive=False\n            )\n            \n            out_final = gr.Markdown(\n                label=\"Differential Diagnosis\"\n            )\n            \n    # Connect logic\n    btn.click(\n        fn=clinical_assessment, \n        inputs=[in_audio, in_image, in_hist], \n        outputs=[out_audio, out_final]\n    )\n\nprint(\"Starting Interface... Click the Public URL below!\")\napp.launch(share=True, debug=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-09T18:08:47.216554Z","iopub.execute_input":"2026-02-09T18:08:47.217300Z","iopub.status.idle":"2026-02-09T18:10:26.709714Z","shell.execute_reply.started":"2026-02-09T18:08:47.217269Z","shell.execute_reply":"2026-02-09T18:10:26.709005Z"}},"outputs":[{"name":"stdout","text":"Starting Interface... Click the Public URL below!\n* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://cadbd7d9649688051b.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://cadbd7d9649688051b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stdout","text":"Keyboard interruption in main thread... closing server.\nKilling tunnel 127.0.0.1:7860 <> https://cadbd7d9649688051b.gradio.live\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}}],"execution_count":10}]}
